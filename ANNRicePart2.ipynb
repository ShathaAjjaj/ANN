{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dbcceaa-c7c1-4582-8b06-1e6725840d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 5 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 5 classes.\n",
      "Using 15000 files for validation.\n",
      "Epoch 1/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.9520 - loss: 0.1408 - val_accuracy: 0.9621 - val_loss: 0.1176\n",
      "Epoch 2/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9723 - loss: 0.0812 - val_accuracy: 0.9668 - val_loss: 0.0973\n",
      "Epoch 3/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9775 - loss: 0.0659 - val_accuracy: 0.9647 - val_loss: 0.1037\n",
      "Epoch 4/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9819 - loss: 0.0539 - val_accuracy: 0.9811 - val_loss: 0.0583\n",
      "Epoch 5/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9835 - loss: 0.0481 - val_accuracy: 0.9872 - val_loss: 0.0402\n",
      "Epoch 6/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9853 - loss: 0.0431 - val_accuracy: 0.9866 - val_loss: 0.0426\n",
      "Epoch 7/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9873 - loss: 0.0376 - val_accuracy: 0.9858 - val_loss: 0.0427\n",
      "Epoch 8/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9877 - loss: 0.0351 - val_accuracy: 0.9869 - val_loss: 0.0407\n",
      "Epoch 9/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9891 - loss: 0.0323 - val_accuracy: 0.9877 - val_loss: 0.0413\n",
      "Epoch 10/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9898 - loss: 0.0295 - val_accuracy: 0.9885 - val_loss: 0.0410\n",
      "Epoch 11/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9911 - loss: 0.0272 - val_accuracy: 0.9868 - val_loss: 0.0461\n",
      "Epoch 12/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9912 - loss: 0.0257 - val_accuracy: 0.9903 - val_loss: 0.0355\n",
      "Epoch 13/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9915 - loss: 0.0246 - val_accuracy: 0.9867 - val_loss: 0.0468\n",
      "Epoch 14/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9915 - loss: 0.0248 - val_accuracy: 0.9795 - val_loss: 0.0699\n",
      "Epoch 15/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9920 - loss: 0.0233 - val_accuracy: 0.9891 - val_loss: 0.0442\n",
      "Epoch 16/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9932 - loss: 0.0203 - val_accuracy: 0.9877 - val_loss: 0.0452\n",
      "Epoch 17/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9927 - loss: 0.0219 - val_accuracy: 0.9894 - val_loss: 0.0369\n",
      "Epoch 18/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9934 - loss: 0.0198 - val_accuracy: 0.9894 - val_loss: 0.0385\n",
      "Epoch 19/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9933 - loss: 0.0196 - val_accuracy: 0.9907 - val_loss: 0.0331\n",
      "Epoch 20/20\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.9935 - loss: 0.0196 - val_accuracy: 0.9863 - val_loss: 0.0476\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x16b4f81a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "import numpy as np\n",
    "\n",
    "# 1. Parameters\n",
    "dataset_path = 'Rice_Image_Dataset' \n",
    "batch_size = 32\n",
    "img_size = (64, 64)\n",
    "\n",
    "# 2. Load dataset (train and validation split)\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    validation_split=0.2,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=img_size,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "\n",
    "# 3. find number of class\n",
    "num_classes = len(train_dataset.class_names)\n",
    "\n",
    "# 4. Normalize pixel values (0-255 to 0-1)\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "val_dataset = val_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "\n",
    "#-----------------------------------------\n",
    "# tf.keras.layers.Rescaling(1./255) creates a layer that divides every pixel value by 255.\n",
    "\n",
    "# Since pixel values range from 0 to 255, dividing by 255 changes the range to 0 to 1.\n",
    "\n",
    "# .map() is a method to apply a function to every element in the dataset.\n",
    "\n",
    "# Here, each element is a tuple (images, labels) where:\n",
    "\n",
    "# images is a batch of images.\n",
    "\n",
    "# labels is the corresponding batch of labels.\n",
    "\n",
    "# lambda x, y: (normalization_layer(x), y) is a short function that:\n",
    "\n",
    "# Takes images x and labels y.\n",
    "\n",
    "# Applies the rescaling (divides all pixels in x by 255).\n",
    "\n",
    "# Returns the normalized images along with the original labels unchanged.\n",
    "#-----------------------------------------\n",
    "\n",
    "# 5. Flatten images for ANN\n",
    "#Dense layers in an ANN expect inputs as 1D vectors, not 3D arrays.\n",
    "#Flattening converts image data into a suitable format for these layers.\n",
    "\n",
    "def flatten_images(images, labels):\n",
    "    # Flatten each image from (batch_size, 64, 64, 3) to (batch_size, 64*64*3)\n",
    "    images = tf.reshape(images, [tf.shape(images)[0], -1])\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(flatten_images)\n",
    "val_dataset = val_dataset.map(flatten_images)\n",
    "\n",
    "# 6. Build ANN model\n",
    "model = Sequential([\n",
    "    Input(shape=(img_size[0] * img_size[1] * 3,)),  # flattened input\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # labels are integers\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 7. Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8ae614-7e8c-4b23-9e61-57794824e1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0476\n",
      "Validation Loss: 0.0476\n",
      "Validation Accuracy: 0.9863\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(val_dataset)\n",
    "print(f'Validation Loss: {loss:.4f}')\n",
    "print(f'Validation Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbaec93-056f-495c-988a-3a28fd23f5ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
